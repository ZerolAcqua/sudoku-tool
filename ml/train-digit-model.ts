import * as tf from '@tensorflow/tfjs-node'
import { createCanvas } from 'canvas'
import * as fs from 'fs'
import * as path from 'path'
import { fileURLToPath } from 'url'
import { PNG } from 'pngjs'

const __dirname = path.dirname(fileURLToPath(import.meta.url))

// ========================
// å·¥å…·å‡½æ•°ï¼šç”Ÿæˆæ•°å­—å­—ä½“å›¾ç‰‡
// ========================

/**
 * ä½¿ç”¨ Canvas ç”Ÿæˆæ•°å­—å­—ä½“å›¾ç‰‡
 * @param digit æ•°å­— 0-9 æˆ– -1 (ä»£è¡¨æ— æ•°å­—)
 * @param width å›¾ç‰‡å®½åº¦
 * @param height å›¾ç‰‡é«˜åº¦
 * @returns Uint8Array ç°åº¦å›¾ç‰‡æ•°æ®
 */
function generateDigitImage(digit: number, width: number = 28, height: number = 28): Uint8Array {
  const canvas = createCanvas(width, height)
  const ctx = canvas.getContext('2d')

  // ç™½è‰²èƒŒæ™¯
  ctx.fillStyle = 'white'
  ctx.fillRect(0, 0, width, height)

  // æ— æ•°å­—æƒ…å†µä¸‹è¿”å›ç©ºç™½å›¾ç‰‡
  if (digit === -1) {
    // è¿”å›å…¨ 255ï¼ˆç™½è‰²ï¼‰çš„ç°åº¦å›¾
    return new Uint8Array(width * height).fill(255)
  }

  // ç»˜åˆ¶æ•°å­—
  ctx.fillStyle = 'black'
  ctx.font = `bold ${Math.floor(width * 0.7)}px Arial`
  ctx.textAlign = 'center'
  ctx.textBaseline = 'middle'
  ctx.fillText(digit.toString(), width / 2, height / 2)

  // è½¬æ¢ä¸ºç°åº¦æ•°æ®
  const imageData = ctx.getImageData(0, 0, width, height)
  const data = imageData.data
  const grayData = new Uint8Array(width * height)

  for (let i = 0; i < width * height; i++) {
    // ä» RGBA è½¬æ¢ä¸ºç°åº¦ (R + G + B) / 3ï¼Œç„¶ååè½¬ä½¿å¾—é»‘è‰²ä¸ºé«˜å€¼
    const idx = i * 4
    const r = data[idx]
    const g = data[idx + 1]
    const b = data[idx + 2]
    const gray = (r + g + b) / 3
    grayData[i] = 255 - gray // åè½¬ä½¿å¾—é»‘è‰²èƒŒæ™¯ä¸ºé«˜å€¼
  }

  return grayData
}

/**
 * æ‰¹é‡ç”Ÿæˆæ•°å­—å­—ä½“æ•°æ®é›†
 * @param samplesPerDigit æ¯ä¸ªæ•°å­—ç”Ÿæˆçš„æ ·æœ¬æ•°
 * @returns { images: Uint8Array, labels: Uint8Array }
 */
function generateSyntheticDataset(samplesPerDigit: number = 100): { images: Uint8Array; labels: number[] } {
  const imageSize = 28
  const numClasses = 11 // 0-9 + æ— æ•°å­—
  const totalSamples = samplesPerDigit * numClasses
  const images = new Uint8Array(totalSamples * imageSize * imageSize)
  const labels: number[] = [] // æ”¹ä¸ºæ™®é€šæ•°ç»„

  console.log(`ç”Ÿæˆåˆæˆæ•°æ®é›†: ${samplesPerDigit} æ ·æœ¬ x ${numClasses} ç±»...`)

  let idx = 0
  for (let digit = 0; digit <= 10; digit++) {
    const actualDigit = digit === 10 ? -1 : digit // æœ€åä¸€ç±»æ˜¯æ— æ•°å­—

    for (let sample = 0; sample < samplesPerDigit; sample++) {
      const digitImage = generateDigitImage(actualDigit, imageSize, imageSize)

      // æ·»åŠ è½»å¾®çš„æ—‹è½¬å’Œç¼©æ”¾å˜åŒ–
      let processedImage = digitImage
      if (actualDigit !== -1 && Math.random() > 0.5) {
        // å¯¹æ•°å­—åº”ç”¨è½»å¾®å˜å½¢
        processedImage = applyTransformation(digitImage, imageSize)
      }

      images.set(processedImage, idx * imageSize * imageSize)
      labels.push(digit)
      idx++
    }

    if ((digit + 1) % 5 === 0) {
      process.stdout.write(`\rå·²ç”Ÿæˆ: ${digit + 1}/${numClasses} ç±»`)
    }
  }
  console.log('\nåˆæˆæ•°æ®é›†ç”Ÿæˆå®Œæˆ!')

  return { images, labels }
}

/**
 * åº”ç”¨è½»å¾®çš„å‡ ä½•å˜æ¢
 */
function applyTransformation(imageData: Uint8Array, size: number): Uint8Array {
  // ç®€å•å®ç°ï¼šè½»å¾®ç¼©æ”¾å’Œä½ç§»
  const transformed = new Uint8Array(size * size)
  const scale = 0.9 + Math.random() * 0.2
  const offsetX = Math.floor((Math.random() - 0.5) * 2)
  const offsetY = Math.floor((Math.random() - 0.5) * 2)

  for (let y = 0; y < size; y++) {
    for (let x = 0; x < size; x++) {
      const srcX = Math.floor((x - offsetX) / scale + size / 2 - size / (2 * scale))
      const srcY = Math.floor((y - offsetY) / scale + size / 2 - size / (2 * scale))

      if (srcX >= 0 && srcX < size && srcY >= 0 && srcY < size) {
        transformed[y * size + x] = imageData[srcY * size + srcX]
      } else {
        transformed[y * size + x] = 255 // èƒŒæ™¯
      }
    }
  }

  return transformed
}

/**
 * åŠ è½½ MNIST æ•°æ®é›†
 * ä½¿ç”¨æœ¬åœ°ä¸‹è½½çš„æ•°æ®æ–‡ä»¶
 */
async function loadMNISTDataset(): Promise<{
  images: tf.Tensor4D
  labels: tf.Tensor2D
}> {
  console.log('åŠ è½½æœ¬åœ° MNIST æ•°æ®é›†...')

  try {
    const IMAGE_SIZE = 784 // 28 * 28
    const NUM_CLASSES = 10
    const NUM_DATASET_ELEMENTS = 65000
    const TRAIN_TEST_RATIO = 5 / 6
    const NUM_TRAIN_ELEMENTS = Math.floor(TRAIN_TEST_RATIO * NUM_DATASET_ELEMENTS)

    const dataDir = path.join(__dirname, 'data')
    const imagesPath = path.join(dataDir, 'mnist_images.png')
    const labelsPath = path.join(dataDir, 'mnist_labels_uint8')

    // åŠ è½½æ ‡ç­¾ï¼ˆäºŒè¿›åˆ¶æ–‡ä»¶ï¼‰
    console.log('åŠ è½½æ ‡ç­¾...')
    const labelsBuffer = fs.readFileSync(labelsPath)
    const datasetLabels = new Uint8Array(labelsBuffer)
    console.log(`âœ… æ ‡ç­¾åŠ è½½å®Œæˆ: ${datasetLabels.length} ä¸ª`)

    // åŠ è½½ PNG å›¾åƒ
    console.log('åŠ è½½ MNIST å›¾åƒ...')
    const imgBuffer = fs.readFileSync(imagesPath)
    
    // ä½¿ç”¨ pngjs è§£æ PNG æ–‡ä»¶
    const png = PNG.sync.read(imgBuffer)
    const img = {
      width: png.width,
      height: png.height,
      data: png.data
    }

    console.log(`âœ… å›¾åƒåŠ è½½å®Œæˆ: ${img.width}x${img.height}`)

    // æå–åƒç´ æ•°æ®ï¼ˆç›´æ¥ä» PNG æ•°æ®ï¼‰
    const pixelData = img.data
    const datasetBytesView = new Float32Array(NUM_DATASET_ELEMENTS * IMAGE_SIZE)

    // PNG æ•°æ®æ˜¯ RGBA æ ¼å¼ï¼Œå°†å…¶è½¬æ¢ä¸ºç°åº¦å€¼
    for (let j = 0; j < IMAGE_SIZE * NUM_DATASET_ELEMENTS; j++) {
      // åªè¯»å–çº¢è‰²é€šé“ï¼ˆç°åº¦å›¾ï¼‰ï¼Œå¹¶å½’ä¸€åŒ–åˆ° [0, 1]
      datasetBytesView[j] = pixelData[j * 4] / 255
    }

    console.log(`âœ… å›¾åƒæ•°æ®æå–å®Œæˆ: ${NUM_DATASET_ELEMENTS} ä¸ªæ ·æœ¬`)

    // è·å–è®­ç»ƒé›†æ•°æ®
    const NUM_TRAIN_LABELS = NUM_TRAIN_ELEMENTS * NUM_CLASSES
    const trainImages = datasetBytesView.slice(0, IMAGE_SIZE * NUM_TRAIN_ELEMENTS)
    const trainLabels = datasetLabels.slice(0, NUM_TRAIN_LABELS)

    console.log(`âœ… MNIST æ•°æ®å‡†å¤‡å®Œæˆ: ${NUM_TRAIN_ELEMENTS} ä¸ªè®­ç»ƒæ ·æœ¬`)

    // è½¬æ¢ä¸ºå¼ é‡
    const imagesTensor = tf.tensor4d(trainImages, [NUM_TRAIN_ELEMENTS, 28, 28, 1])
    const labelsTensor = tf.tensor2d(trainLabels, [NUM_TRAIN_ELEMENTS, NUM_CLASSES])

    return { images: imagesTensor, labels: labelsTensor }
  } catch (err) {
    console.warn('âš ï¸  MNIST æ•°æ®åŠ è½½å¤±è´¥:', err instanceof Error ? err.message : err)
    console.log('è¯·ç¡®ä¿ mnist_images.png å’Œ mnist_labels_uint8 åœ¨ ml/data ç›®å½•ä¸­')
    // è¿”å›ç©ºå¼ é‡
    return {
      images: tf.tensor4d([], [0, 28, 28, 1]),
      labels: tf.tensor2d([], [0, 10])
    }
  }
}

/**
 * åªä½¿ç”¨ MNIST æ•°æ®é›†
 */
async function createMNISTDataset(): Promise<{
  trainImages: tf.Tensor4D
  trainLabels: tf.Tensor2D
  testImages: tf.Tensor4D
  testLabels: tf.Tensor2D
}> {
  console.log('åŠ è½½ MNIST æ•°æ®é›†...')
  const mnistData = await loadMNISTDataset()

  if (mnistData.images.shape[0] === 0) {
    throw new Error('MNIST æ•°æ®åŠ è½½å¤±è´¥')
  }

  // è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ (samples, 28, 28, 1)
  let allImages = mnistData.images as tf.Tensor4D
  let allLabels = mnistData.labels as tf.Tensor2D

  console.log(`åŠ è½½å®Œæˆ: ${allImages.shape[0]} ä¸ªæ ·æœ¬`)

  // æ‰“ä¹±æ•°æ®
  const indices = tf.util.createShuffledIndices(allImages.shape[0])
  const indicesTensor = tf.tensor1d(Array.from(indices), 'int32')
  allImages = tf.gather(allImages, indicesTensor, 0) as tf.Tensor4D
  allLabels = tf.gather(allLabels, indicesTensor, 0) as tf.Tensor2D
  indicesTensor.dispose()

  // åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›† (80/20)
  const trainSize = Math.floor(allImages.shape[0] * 0.8)
  const trainImages = allImages.slice([0, 0, 0, 0], [trainSize, 28, 28, 1]) as tf.Tensor4D
  const trainLabels = allLabels.slice([0, 0], [trainSize, 10]) as tf.Tensor2D
  const testImages = allImages.slice([trainSize, 0, 0, 0], [-1, 28, 28, 1]) as tf.Tensor4D
  const testLabels = allLabels.slice([trainSize, 0], [-1, 10]) as tf.Tensor2D

  console.log(`è®­ç»ƒé›†: ${trainImages.shape[0]} ä¸ªæ ·æœ¬`)
  console.log(`æµ‹è¯•é›†: ${testImages.shape[0]} ä¸ªæ ·æœ¬`)

  return { trainImages, trainLabels, testImages, testLabels }
}

// ========================
// æ„å»ºå’Œè®­ç»ƒæ¨¡å‹
// ========================

async function trainDigitModel() {
  console.log('å¼€å§‹è®­ç»ƒæ•°å­—è¯†åˆ«æ¨¡å‹...\n')
  console.log('TensorFlow.js ç‰ˆæœ¬:', tf.version)
  console.log('å·¥ä½œç›®å½•:', __dirname)
  console.log('')

  // åˆ›å»ºè¾“å‡ºç›®å½•
  const modelsDir = path.join(__dirname, '../public/models')
  if (!fs.existsSync(modelsDir)) {
    fs.mkdirSync(modelsDir, { recursive: true })
    console.log(`åˆ›å»ºç›®å½•: ${modelsDir}`)
  }

  // åˆ›å»ºä¸­é—´æ¨¡å‹æ£€æŸ¥ç‚¹ç›®å½•
  const checkpointDir = path.join(__dirname, 'models')
  if (!fs.existsSync(checkpointDir)) {
    fs.mkdirSync(checkpointDir, { recursive: true })
    console.log(`åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•: ${checkpointDir}`)
  }

  // åˆ›å»ºæ•°æ®é›†
  const { trainImages, trainLabels, testImages, testLabels } = await createMNISTDataset()

  // æ„å»ºæ¨¡å‹
  const model = tf.sequential({
    layers: [
      tf.layers.conv2d({
        inputShape: [28, 28, 1],
        filters: 32,
        kernelSize: 3,
        activation: 'relu',
        padding: 'same',
      }),
      tf.layers.batchNormalization(),
      tf.layers.maxPooling2d({ poolSize: 2 }),
      tf.layers.dropout({ rate: 0.25 }),

      tf.layers.conv2d({
        filters: 64,
        kernelSize: 3,
        activation: 'relu',
        padding: 'same',
      }),
      tf.layers.batchNormalization(),
      tf.layers.maxPooling2d({ poolSize: 2 }),
      tf.layers.dropout({ rate: 0.25 }),

      tf.layers.flatten(),
      tf.layers.dense({
        units: 128,
        activation: 'relu',
      }),
      tf.layers.batchNormalization(),
      tf.layers.dropout({ rate: 0.5 }),
      tf.layers.dense({
        units: 10, // 10 ä¸ªåˆ†ç±»ï¼š0-9
        activation: 'softmax',
      }),
    ],
  })

  model.compile({
    optimizer: tf.train.adam(0.001),
    loss: 'categoricalCrossentropy',
    metrics: ['accuracy'],
  })

  console.log('æ¨¡å‹ç»“æ„:')
  model.summary()
  console.log('\n')

  // è®­ç»ƒæ¨¡å‹
  const CHECKPOINT_INTERVAL = 1 // æ¯ 1 ä¸ª epoch ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
  const history = await model.fit(trainImages, trainLabels, {
    batchSize: 128,
    epochs: 10,
    validationData: [testImages, testLabels],
    shuffle: true,
    callbacks: {
      onEpochEnd: async (epoch, logs) => {
        console.log(
          `Epoch ${epoch + 1}/10 - loss: ${logs?.loss?.toFixed(4)}, ` +
            `accuracy: ${logs?.acc?.toFixed(4)}, ` +
            `val_loss: ${logs?.val_loss?.toFixed(4)}, ` +
            `val_accuracy: ${logs?.val_acc?.toFixed(4)}`
        )

        // ä¿å­˜æ£€æŸ¥ç‚¹
        if ((epoch + 1) % CHECKPOINT_INTERVAL === 0) {
          const checkpointPath = 'file://' + path.join(checkpointDir, `checkpoint-epoch-${epoch + 1}`)
          console.log(`ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: ${checkpointPath}`)
          await model.save(checkpointPath)
        }
      },
    },
  })

  // è¯„ä¼°æ¨¡å‹
  const evalResult = model.evaluate(testImages, testLabels) as tf.Scalar[]
  console.log(`\næœ€ç»ˆæµ‹è¯•æŸå¤±: ${evalResult[0].dataSync()[0].toFixed(4)}`)
  console.log(`æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: ${evalResult[1].dataSync()[0].toFixed(4)}`)

  // ä¿å­˜æ¨¡å‹
  const modelPath = 'file://' + path.join(__dirname, '../public/models/sudoku-digit')
  console.log(`\nä¿å­˜æ¨¡å‹åˆ°: ${modelPath}`)
  await model.save(modelPath)

  console.log('è®­ç»ƒå®Œæˆï¼')

  // æ¸…ç†èµ„æº
  model.dispose()
  trainImages.dispose()
  trainLabels.dispose()
  testImages.dispose()
  testLabels.dispose()
}

// è¿è¡Œè®­ç»ƒ
trainDigitModel()
  .then(() => {
    console.log('âœ… è®­ç»ƒæˆåŠŸå®Œæˆ')
    process.exit(0)
  })
  .catch(err => {
    console.error('âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™:')
    console.error(err)
    if (err.stack) {
      console.error(err.stack)
    }
    process.exit(1)
  })
